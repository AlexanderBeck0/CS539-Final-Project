{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8146e7c",
   "metadata": {},
   "source": [
    "# Song Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb95ba",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Data can be found here: https://www.kaggle.com/datasets/tonygordonjr/spotify-dataset-2023?select=spotify_data_12_20_2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab39b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from Assignment1Tree import Tree\n",
    "\n",
    "# Note that the converters section makes it take literally twice as long to load\n",
    "data: pd.DataFrame = pd.read_csv(\n",
    "    \"spotify_data_12_20_2023.csv\",\n",
    "    converters={\n",
    "        \"artists\": ast.literal_eval,  # Makes it so that artists becomes an actual list instead of a string representation of one\n",
    "        # \"artist_genres\": ast.literal_eval,\n",
    "    },\n",
    "    dtype={\n",
    "        \"album_name\": \"string\",\n",
    "        \"name\": \"string\",\n",
    "        \"track_name\": \"string\",\n",
    "        \"explicit\": \"boolean\",\n",
    "        \"album_type\": \"category\",\n",
    "        \"genre_0\": \"string\",\n",
    "        \"genre_1\": \"string\",\n",
    "        \"genre_2\": \"string\",\n",
    "        \"genre_3\": \"string\",\n",
    "        \"genre_4\": \"string\",\n",
    "    },\n",
    "    parse_dates=[\"release_date\"],\n",
    ")\n",
    "\n",
    "if \"duration_ms\" in data.columns and \"duration_sec\" in data.columns:\n",
    "    data.drop(columns=[\"duration_sec\"], inplace=True)\n",
    "\n",
    "# TODO: Drop rows SELECTIVELY\n",
    "# print(data.isna().sum())\n",
    "hardcoded_drop_columns: list[str] = [\n",
    "    \"rn\",\n",
    "    \"uri\",\n",
    "    \"track_number\",\n",
    "    \"analysis_url\",\n",
    "    \"track_href\",\n",
    "    \"album_id\",\n",
    "    \"artist_0\",\n",
    "    \"artist_1\",\n",
    "    \"artist_2\",\n",
    "    \"artist_3\",\n",
    "    \"artist_4\",\n",
    "    \"artist_id\",\n",
    "    \"type\",\n",
    "    \"total_tracks\",\n",
    "]\n",
    "data.drop(\n",
    "    columns=[c for c in hardcoded_drop_columns if c in data.columns], inplace=True\n",
    ")\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "data.dropna(\n",
    "    axis=0,\n",
    "    subset=[\n",
    "        \"track_popularity\",\n",
    "        \"tempo\",\n",
    "        \"acousticness\",\n",
    "        \"danceability\",\n",
    "        \"release_year\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# BUG: DtypeWarning: Columns (25,26,44) have mixed types\n",
    "\n",
    "# Professor suggestions:\n",
    "####\n",
    "# Split it into \"model\" trees\n",
    "# Figure out if there are bounds where it is extremely accurate\n",
    "# Find a cutoff\n",
    "\n",
    "# Pick a random 10k instead of 300k\n",
    "# Calculate a balanced accuracy\n",
    "# The balanced accuracy as a function of split is a curve that peaks somewhere\n",
    "# If it is smooth then you can do a binary search\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78edb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the percentage of the track_popularities with different thresholds\n",
    "cumulative_percent = 0\n",
    "for i in range(50, 70):\n",
    "    y = (data[\"track_popularity\"] > i).astype(int)\n",
    "    percent_popular = (y == 1).sum() / len(y) * 100\n",
    "    cumulative_percent += percent_popular\n",
    "\n",
    "    # Show the distribution of the track_popularity on the above threshold\n",
    "    print(f\"{i}: {percent_popular:0.2f} ({cumulative_percent:0.2f} total)\")\n",
    "\n",
    "# [40, 100] has 93% of data as \"popular\"\n",
    "# [50, 100] has 41%\n",
    "# [60, 100] has 16%\n",
    "# [70, 100] has 4.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f58fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize genres\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "if \"genres\" not in data.columns:\n",
    "    data[\"genres\"] = (\n",
    "        data[[\"genre_0\", \"genre_1\", \"genre_2\", \"genre_3\", \"genre_4\"]]\n",
    "        .fillna(\"\")\n",
    "        .agg(\" \".join, axis=1)\n",
    "    )\n",
    "\n",
    "    data.drop(\n",
    "        columns=[\"genre_0\", \"genre_1\", \"genre_2\", \"genre_3\", \"genre_4\"], inplace=True\n",
    "    )\n",
    "\n",
    "NUM_SVD_COMPONENTS = 10\n",
    "TFIDF_PICKLE_PATH = \"tfidf.pkl\"\n",
    "SVD_PICKLE_PATH = \"svd.pkl\"\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "svd = TruncatedSVD(n_components=NUM_SVD_COMPONENTS, random_state=2)\n",
    "tfidf_vectors = None\n",
    "decomposed_tfidf = None\n",
    "if os.path.exists(TFIDF_PICKLE_PATH) and os.path.exists(SVD_PICKLE_PATH):\n",
    "    # Load the pickle\n",
    "    with open(TFIDF_PICKLE_PATH, \"rb\") as tfidf_file:\n",
    "        tfidf_vectors = pickle.load(tfidf_file)\n",
    "    with open(SVD_PICKLE_PATH, \"rb\") as svd_file:\n",
    "        decomposed_tfidf = pickle.load(svd_file)\n",
    "else:\n",
    "    # Create vectors and save the pickle\n",
    "    tfidf_vectors = tfidf.fit_transform(data[\"genres\"])\n",
    "    decomposed_tfidf = svd.fit_transform(tfidf_vectors)\n",
    "    with open(TFIDF_PICKLE_PATH, \"wb\") as tfidf_file:\n",
    "        pickle.dump(tfidf_vectors, tfidf_file)\n",
    "    with open(SVD_PICKLE_PATH, \"wb\") as svd_file:\n",
    "        pickle.dump(decomposed_tfidf, svd_file)\n",
    "\n",
    "assert tfidf_vectors is not None, \"Failed to load tfidf pickle!\"\n",
    "assert decomposed_tfidf is not None, \"Failed to load decomposed tfidf pickle!\"\n",
    "\n",
    "for i in range(decomposed_tfidf.shape[1]):\n",
    "    data[f\"genre_vector_{i}\"] = decomposed_tfidf[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2eeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data is not None\n",
    "\n",
    "x = data.select_dtypes(include=\"number\", exclude=\"int64\").drop(\n",
    "    columns=[\"track_popularity\"]\n",
    ")\n",
    "\n",
    "# TODO: Train the track_popularity number\n",
    "y = (data[\"track_popularity\"] > 50).astype(int)\n",
    "\n",
    "# Show the distribution of the track_popularity on the above threshold\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834ed26",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49be3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert data is not None\n",
    "\n",
    "# No MAX_DEPTH leads to higher accuracy (0.6361 -> 0.9147), but it has a lower AUC (0.7587 -> 0.5767)\n",
    "# My guess is, overfitting with no max depth.\n",
    "MAX_DEPTH: int | None = 10\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=MAX_DEPTH,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=x.size,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",  # Make the 1's have more hold\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.1, random_state=x.size\n",
    ")\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(x_test)\n",
    "y_pred_proba = dt.predict_proba(x_test)[:, 1]  # type: ignore\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "area_under_curve = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:0.4f}\")\n",
    "print(f\"Area Under Curve: {area_under_curve:0.4f}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(y_test, y_pred):0.4f}\")\n",
    "\n",
    "\n",
    "FEATURE = \"loudness\"\n",
    "assert FEATURE in x.columns\n",
    "\n",
    "# Random sample the data for visualization\n",
    "SAMPLE_SIZE = min(3000, len(x_test))\n",
    "sample_index = np.random.choice(len(x_test), size=SAMPLE_SIZE, replace=False)\n",
    "\n",
    "x_sample = x_test.iloc[sample_index]\n",
    "y_sample = y_test.iloc[sample_index]\n",
    "y_pred_sample = y_pred[sample_index]\n",
    "print(y_pred_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0366e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#plot-the-results\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    x_sample[FEATURE],\n",
    "    y_sample,\n",
    "    s=20,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"none\",\n",
    "    c=\"black\",\n",
    "    label=\"Data\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x_sample[FEATURE],\n",
    "    y_pred_sample,\n",
    "    color=\"red\",\n",
    "    s=15,\n",
    "    alpha=1,\n",
    "    label=f\"Predicted Data (max_depth={MAX_DEPTH})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(FEATURE)\n",
    "plt.ylabel(\"track_popularity\")\n",
    "plt.title(\"Decision Tree Classifier\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, export_graphviz\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "# TODO: I'm not sure the order in which Popular/Unpopular should be\n",
    "# They are DIFFERENT between the export and this one\n",
    "plot_tree(\n",
    "    dt,\n",
    "    feature_names=x.columns.tolist(),\n",
    "    class_names=[\"Popular\", \"Unpopular\"],\n",
    "    fontsize=10,\n",
    "    filled=True,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "EXPORT_DT = False\n",
    "if EXPORT_DT:\n",
    "    export_graphviz(\n",
    "        dt,\n",
    "        out_file=\"decision_tree.dot\",\n",
    "        feature_names=x.columns.tolist(),\n",
    "        class_names=[\"Unpopular\", \"Popular\"],\n",
    "        filled=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete OR refactor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This was just some test code to see how the model is doing with a different, more modern dataset\n",
    "# https://www.kaggle.com/datasets/solomonameh/spotify-music-dataset?select=high_popularity_spotify_data.csv\n",
    "temp_df = pd.read_csv(\n",
    "    \"high_popularity_spotify_data.csv\",\n",
    "    dtype={\n",
    "        \"track_popularity\": \"float\",\n",
    "        \"duration_ms\": \"float\",\n",
    "        \"mode\": \"float\",\n",
    "        \"time_signature\": \"float\",\n",
    "        \"key\": \"float\",\n",
    "    },\n",
    ")\n",
    "\n",
    "if \"genres\" not in temp_df.columns:\n",
    "    temp_df[\"genres\"] = (\n",
    "        temp_df[[\"playlist_genre\", \"playlist_subgenre\"]]\n",
    "        .fillna(\"\")\n",
    "        .agg(\" \".join, axis=1)\n",
    "    )\n",
    "\n",
    "    temp_df.drop(columns=[\"playlist_genre\", \"playlist_subgenre\"], inplace=True)\n",
    "\n",
    "combined = pd.concat([data[\"genres\"], temp_df[\"genres\"]])\n",
    "# tfidf.fit(combined)\n",
    "temp_tfidf_vectors = tfidf.transform(temp_df[\"genres\"])\n",
    "\n",
    "temp_decomposed_tfidf = svd.transform(temp_tfidf_vectors)\n",
    "\n",
    "for i in range(temp_decomposed_tfidf.shape[1]):\n",
    "    temp_df[f\"genre_vector_{i}\"] = temp_decomposed_tfidf[:, i]\n",
    "\n",
    "# Map all the columns from another dataset to this one\n",
    "column_mapping = {\n",
    "    \"track_id\": \"track_id\",\n",
    "    \"track_name\": \"track_name\",\n",
    "    \"track_album_name\": \"album_name\",\n",
    "    \"track_album_id\": \"album_id\",\n",
    "    \"track_popularity\": \"track_popularity\",\n",
    "    \"track_album_release_date\": \"release_date\",\n",
    "    \"energy\": \"energy\",\n",
    "    \"tempo\": \"tempo\",\n",
    "    \"danceability\": \"danceability\",\n",
    "    \"loudness\": \"loudness\",\n",
    "    \"liveness\": \"liveness\",\n",
    "    \"valence\": \"valence\",\n",
    "    \"speechiness\": \"speechiness\",\n",
    "    \"acousticness\": \"acousticness\",\n",
    "    \"instrumentalness\": \"instrumentalness\",\n",
    "    \"mode\": \"mode\",\n",
    "    \"key\": \"key\",\n",
    "    \"time_signature\": \"time_signature\",\n",
    "    \"duration_ms\": \"duration_ms\",\n",
    "    \"genres\": \"genres\",\n",
    "}\n",
    "for i in range(0, temp_decomposed_tfidf.shape[1]):\n",
    "    column_mapping[f\"genre_vector_{i}\"] = f\"genre_vector_{i}\"\n",
    "    \n",
    "temp_df = temp_df.rename(columns=column_mapping)[list(column_mapping.values())]\n",
    "\n",
    "if \"release_date\" in temp_df.columns:\n",
    "    # Get release_year from release_date\n",
    "    temp_df[\"release_year\"] = (\n",
    "        temp_df[\"release_date\"].astype(str).str.extract(r\"(\\d{4})\").astype(float)\n",
    "    )\n",
    "\n",
    "# Add missing columns as nan\n",
    "for data_column in data.columns:\n",
    "    if data_column not in temp_df.columns:\n",
    "        temp_df[data_column] = np.nan\n",
    "\n",
    "\n",
    "# Only keep data.columns values\n",
    "temp_df = temp_df[data.columns]\n",
    "temp_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "temp_df.drop(\n",
    "    columns=[c for c in hardcoded_drop_columns if c in temp_df.columns], inplace=True\n",
    ")\n",
    "\n",
    "temp_df.drop(\n",
    "    columns=[\n",
    "        \"album_popularity\",\n",
    "        \"album_type\",\n",
    "        \"artists\",\n",
    "        \"label\",\n",
    "        \"artist_genres\",\n",
    "        \"artist_popularity\",\n",
    "        \"followers\",\n",
    "        \"name\",\n",
    "        \"explicit\",\n",
    "        \"release_month\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "temp_df.dropna(\n",
    "    axis=0,\n",
    "    subset=[\n",
    "        \"track_popularity\",\n",
    "        \"tempo\",\n",
    "        \"acousticness\",\n",
    "        \"danceability\",\n",
    "        \"release_year\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp = temp_df.select_dtypes(include=\"number\", exclude=\"int64\").drop(\n",
    "    columns=[\"track_popularity\"]\n",
    ")\n",
    "\n",
    "# TODO: Train the track_popularity number\n",
    "y_temp = (temp_df[\"track_popularity\"] > 50).astype(int)\n",
    "# dt.predict(x_temp)\n",
    "results = dt.predict(x_temp)\n",
    "# temp_df.head()\n",
    "# x_temp.head()\n",
    "print(np.unique_counts(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9327224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree AUC\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "# A good decision tree will have a HIGH AUC (0.8+), high TPR with low FPR.\n",
    "plt.figure()\n",
    "rcd: RocCurveDisplay = RocCurveDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Decision Tree AUC\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93242fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  # For creating parameter combinations\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "# Grid Search Outline\n",
    "# TODO: Change max_depth to be based on NUM_SVD_COMPONENTS instead of 3, 5, 7, 10, 15\n",
    "param_grid = {  # Define the parameter grid for hyperparameter tuning\n",
    "    \"max_depth\": [3, 5, 7, 10, 15, None],\n",
    "    \"MAX_SVD_COMPONENTS\": [5, 10, 15],\n",
    "    \"splitting_point\": [\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        70,\n",
    "    ],  # starting from 40 to 70 based on previous analysis, with gaps for efficiency during debugging\n",
    "}\n",
    "\n",
    "\n",
    "param_keys = list(param_grid.keys())\n",
    "param_values = list(param_grid.values())\n",
    "param_combinations = list(itertools.product(*param_values))\n",
    "print(f\"Testing {len(param_combinations)} combinations...\")\n",
    "print(param_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "from typing import Any\n",
    "\n",
    "results: list[dict[str, Any]] = []\n",
    "\n",
    "x = data.select_dtypes(include=\"number\", exclude=\"int64\").drop(\n",
    "    columns=[\"track_popularity\"]\n",
    ")\n",
    "\n",
    "for combo in param_combinations:\n",
    "    max_depth, cutoff = combo\n",
    "\n",
    "    # Data with current cutoff\n",
    "    y = (data[\"track_popularity\"] > cutoff).astype(int)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.1, random_state=x.size\n",
    "    )\n",
    "\n",
    "    dt = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=3,\n",
    "        random_state=x.size,\n",
    "        max_features=\"sqrt\",\n",
    "        class_weight=\"balanced\",\n",
    "    )\n",
    "\n",
    "    dt.fit(x_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = dt.predict(x_test)\n",
    "    y_pred_proba = dt.predict_proba(x_test)[:, 1]  # type: ignore\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    area_under_curve = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Save results\n",
    "    results.append(\n",
    "        {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"cutoff\": cutoff,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"auc\": area_under_curve,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "SORT_BY = \"auc\"\n",
    "assert SORT_BY in [\n",
    "    \"max_depth\",\n",
    "    \"cutoff\",\n",
    "    \"accuracy\",\n",
    "    \"auc\",\n",
    "]  # To see what you can sort by\n",
    "results_df = results_df.sort_values(SORT_BY, ascending=False)\n",
    "\n",
    "print(\"-\" * 25)\n",
    "print(f\"Best results by {SORT_BY.upper()}\")\n",
    "print(\"-\" * 25)\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "best_params = results_df.iloc[0]\n",
    "for column in results_df.columns:\n",
    "    print(f\"{column.replace('_', ' ').capitalize()}: {best_params[column]:0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
